{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "disp_freq = 100\n",
    "\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "# Create model\n",
    "def conv_net(x, weights, biases):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "def norm(l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n",
    "\n",
    "def alex_net(x,weights,biases):\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "    conv1 = norm(conv1,lsize=4) \n",
    "    \n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "    conv2 = norm(conv2,lsize=4)\n",
    "    \n",
    "    conv3 = conv2d(conv2, weights['wc3'], biases['bc3'])\n",
    "    conv3 = maxpool2d(conv3,k=2)\n",
    "    conv3 = norm(conv3,lsize=4)\n",
    "    \n",
    "    fc1 = tf.reshape(conv3, [-1, weights['wd1'].get_shape().as_list()[0]]) \n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    fc2 = tf.nn.relu(tf.matmul(fc1, weights['wd2']) + biases['bd2']) \n",
    "    out = tf.add(tf.matmul(fc2, weights['out']), biases['out'])\n",
    "    return out\n",
    "    \n",
    "class Model:\n",
    "\n",
    "\n",
    "    def __init__(self, x, y_):\n",
    "\n",
    "        self.x = x # input placeholder\n",
    "    \n",
    "        weights = {\n",
    "        'wc1': tf.Variable(tf.truncated_normal([3, 3, 1, 64],stddev=0.001)),\n",
    "        'wc2': tf.Variable(tf.truncated_normal([3, 3, 64, 128],stddev=0.001)),\n",
    "        'wc3': tf.Variable(tf.truncated_normal([3, 3, 128, 256],stddev=0.001)),\n",
    "        'wd1': tf.Variable(tf.truncated_normal([4*4*256, 1024],stddev=0.001)),\n",
    "        'wd2': tf.Variable(tf.truncated_normal([1024, 1024],stddev=0.001)),\n",
    "        'out': tf.Variable(tf.truncated_normal([1024, 10],stddev=0.001))\n",
    "        }\n",
    "        biases = {\n",
    "        'bc1': tf.Variable(tf.truncated_normal([64],stddev=0.001)),\n",
    "        'bc2': tf.Variable(tf.truncated_normal([128],stddev=0.001)),\n",
    "        'bc3': tf.Variable(tf.truncated_normal([256],stddev=0.001)),\n",
    "        'bd1': tf.Variable(tf.truncated_normal([1024],stddev=0.001)),\n",
    "        'bd2': tf.Variable(tf.truncated_normal([1024],stddev=0.001)),\n",
    "        'out': tf.Variable(tf.truncated_normal([n_classes],stddev=0.001))\n",
    "        }\n",
    "        \n",
    "        self.y = alex_net(x, weights, biases)\n",
    "        self.var_list=[weights['wc1'],weights['wc2'],weights['wc3'],weights['wd1'],weights['wd2'],weights['out'],\n",
    "                       biases['bc1'],biases['bc2'],biases['bc3'],biases['bd1'],biases['bd2'],biases['out']]\n",
    "#         weights = {\n",
    "#         # 5x5 conv, 1 input, 32 outputs\n",
    "#         'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32],stddev=0.001)),\n",
    "#         # 5x5 conv, 32 inputs, 64 outputs\n",
    "#         'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64],stddev=0.001)),\n",
    "#         # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "#         'wd1': tf.Variable(tf.truncated_normal([7*7*64, 1024],stddev=0.001)),\n",
    "#         # 1024 inputs, 10 outputs (class prediction)\n",
    "#         'out': tf.Variable(tf.truncated_normal([1024, n_classes],stddev=0.001))\n",
    "#         }\n",
    "\n",
    "#         biases = {\n",
    "#         'bc1': tf.Variable(tf.truncated_normal([32],stddev=0.001)),\n",
    "#         'bc2': tf.Variable(tf.truncated_normal([64],stddev=0.001)),\n",
    "#         'bd1': tf.Variable(tf.truncated_normal([1024],stddev=0.001)),\n",
    "#         'out': tf.Variable(tf.truncated_normal([n_classes],stddev=0.001))\n",
    "#         }\n",
    "       \n",
    "        \n",
    "#         self.y = conv_net(x, weights, biases)\n",
    "\n",
    "#         self.var_list=[weights['wc1'],weights['wc2'],weights['wd1'],weights['out'],\n",
    "#                        biases['bc1'],biases['bc2'],biases['bd1'],biases['out']]\n",
    "\n",
    "        # simple 2-layer network\n",
    "#         in_dim = int(x.get_shape()[1]) # 784 for MNIST\n",
    "#         out_dim = int(y_.get_shape()[1]) # 10 for MNIST\n",
    "\n",
    "#         W1 = weight_variable([in_dim,50])\n",
    "#         b1 = bias_variable([50])\n",
    "\n",
    "#         W2 = weight_variable([50,out_dim])\n",
    "#         b2 = bias_variable([out_dim])\n",
    "\n",
    "#         h1 = tf.nn.relu(tf.matmul(x,W1) + b1) # hidden layer    \n",
    "#         self.y = tf.matmul(h1,W2) + b2\n",
    "#         self.var_list = [W1, b1, W2, b2]\n",
    "        \n",
    "        \n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y,labels=y_))\n",
    "\n",
    "        self.set_vanilla_loss()\n",
    "        \n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(y_,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "\n",
    "    def compute_fisher(self, imgset, sess, num_samples=200, plot_diffs=False, disp_freq=10):\n",
    "        # compute Fisher information for each parameter\n",
    "\n",
    "        # initialize Fisher information for most recent task\n",
    "        self.F_accum = []\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.F_accum.append(np.zeros(self.var_list[v].get_shape().as_list()))\n",
    "\n",
    "        # sampling a random class from softmax\n",
    "        probs = tf.nn.softmax(self.y)\n",
    "        class_ind = tf.to_int32(tf.multinomial(tf.log(probs), 1)[0][0])\n",
    "\n",
    "        if(plot_diffs):\n",
    "            # track differences in mean Fisher info\n",
    "            F_prev = deepcopy(self.F_accum)\n",
    "            mean_diffs = np.zeros(0)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # select random input image\n",
    "            im_ind = np.random.randint(imgset.shape[0])\n",
    "            # compute first-order derivatives\n",
    "            ders = sess.run(tf.gradients(tf.log(probs[0,class_ind]), self.var_list), feed_dict={self.x: imgset[im_ind:im_ind+1]})\n",
    "            # square the derivatives and add to total\n",
    "            for v in range(len(self.F_accum)):\n",
    "                self.F_accum[v] += np.square(ders[v])\n",
    "            if(plot_diffs):\n",
    "                if i % disp_freq == 0 and i > 0:\n",
    "                    # recording mean diffs of F\n",
    "                    F_diff = 0\n",
    "                    for v in range(len(self.F_accum)):\n",
    "                        F_diff += np.sum(np.absolute(self.F_accum[v]/(i+1) - F_prev[v]))\n",
    "                    mean_diff = np.mean(F_diff)\n",
    "                    mean_diffs = np.append(mean_diffs, mean_diff)\n",
    "                    for v in range(len(self.F_accum)):\n",
    "                        F_prev[v] = self.F_accum[v]/(i+1)\n",
    "                    plt.plot(range(disp_freq+1, i+2, disp_freq), mean_diffs)\n",
    "                    plt.xlabel(\"Number of samples\")\n",
    "                    plt.ylabel(\"Mean absolute Fisher difference\")\n",
    "                    display.display(plt.gcf())\n",
    "                    display.clear_output(wait=True)\n",
    "\n",
    "        # divide totals by number of samples\n",
    "        for v in range(len(self.F_accum)):\n",
    "            self.F_accum[v] /= num_samples\n",
    "    \n",
    "    def set_vanilla_loss(self):\n",
    "        self.train_step = tf.train.AdamOptimizer(0.01).minimize(self.cross_entropy)\n",
    "\n",
    "    def star(self):\n",
    "    # used for saving optimal weights after most recent task training\n",
    "        self.star_vars = []\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.star_vars.append(self.var_list[v].eval())\n",
    "    def update_ewc_loss(self, lam):\n",
    "        # elastic weight consolidation\n",
    "        # lam is weighting for previous task(s) constraints\n",
    "\n",
    "        if not hasattr(self, \"ewc_loss\"):\n",
    "            self.ewc_loss = self.cross_entropy\n",
    "\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.ewc_loss += (lam/2) * tf.reduce_sum(tf.multiply(self.F_accum[v].astype(np.float32),tf.square(self.var_list[v] - self.star_vars[v])))\n",
    "        self.train_step = tf.train.AdamOptimizer(0.01).minimize(self.ewc_loss)\n",
    "    def restore(self, sess):\n",
    "    # reassign optimal weights for latest task\n",
    "        if hasattr(self, \"star_vars\"):\n",
    "            for v in range(len(self.var_list)):\n",
    "                sess.run(self.var_list[v].assign(self.star_vars[v]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Model(x, y_)\n",
    "\n",
    "#print model.getVarList()\n",
    "#sess = tf.InteractiveSession()\n",
    "#sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_imshow(img):\n",
    "    plt.imshow(img.reshape([28,28]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "def permute_mnist(mnist):\n",
    "    perm_inds = range(mnist.train.images.shape[1])\n",
    "    np.random.shuffle(perm_inds)\n",
    "    mnist2 = deepcopy(mnist)\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "    for set_name in sets:\n",
    "        this_set = getattr(mnist2, set_name) # shallow copy\n",
    "        this_set._images = np.transpose(np.array([this_set.images[:,c] for c in perm_inds]))\n",
    "    return mnist2\n",
    "\n",
    "def plot_test_acc(plot_handles):\n",
    "    plt.legend(handles=plot_handles, loc=\"center right\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.ylim(0,1)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_task(model, num_iter, disp_freq,sess, trainset, testsets, x, y_, lams=[0]):\n",
    "    for l in range(len(lams)):\n",
    "        # lams[l] sets weight on old task(s)\n",
    "        temp = set(tf.global_variables())\n",
    "        model.restore(sess) # reassign optimal weights from previous training session\n",
    "        if(lams[l] == 0):\n",
    "            model.set_vanilla_loss()\n",
    "        else:\n",
    "            model.update_ewc_loss(lams[l])\n",
    "            #model.train_step = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        # initialize test accuracy array for each task \n",
    "        #Optimizer = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))\n",
    "        \n",
    "        test_accs = []\n",
    "        for task in range(len(testsets)):\n",
    "            test_accs.append(np.zeros(num_iter/disp_freq))\n",
    "        # train on current task\n",
    "        step = 0\n",
    "        for iter in range(num_iter):\n",
    "            batch = trainset.train.next_batch(batch_size)\n",
    "            model.train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            #print model.prediction.eval(feed_dict={x: mnist.test.images})\n",
    "            if (iter % disp_freq == 0):\n",
    "                plt.subplot(1, len(lams), l+1)\n",
    "                plots = []\n",
    "                colors = ['r', 'b', 'g']\n",
    "                for task in range(len(testsets)):\n",
    "                    feed_dict={x: testsets[task].test.images, y_: testsets[task].test.labels}\n",
    "                    test_accs[task][iter/disp_freq] = model.accuracy.eval(feed_dict=feed_dict)\n",
    "                    c = chr(ord('A') + task)\n",
    "                    plot_h, = plt.plot(range(1,iter+2,disp_freq), test_accs[task][:iter/disp_freq+1], colors[task], label=\"task \" + c)\n",
    "                    plots.append(plot_h)\n",
    "                plot_test_acc(plots)\n",
    "                if l == 0: \n",
    "                    plt.title(\"Cross Entropy\")\n",
    "                else:\n",
    "                    plt.title(\"ewc\")\n",
    "                plt.gcf().set_size_inches(len(lams)*5, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_task_separate(model, num_iter, disp_freq,sess, trainset,setnum, testsets, x, y_, lams=[0]):\n",
    "    for l in range(len(lams)):\n",
    "        # lams[l] sets weight on old task(s)\n",
    "        temp = set(tf.global_variables())\n",
    "        model.restore(sess) # reassign optimal weights from previous training session\n",
    "        if(lams[l] == 0):\n",
    "            model.set_vanilla_loss()\n",
    "        else:\n",
    "            model.update_ewc_loss(lams[l])\n",
    "            #model.train_step = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        # initialize test accuracy array for each task \n",
    "        #Optimizer = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))\n",
    "        \n",
    "        test_accs = []\n",
    "        for task in range(len(testsets)):\n",
    "            test_accs.append(np.zeros(num_iter/disp_freq))\n",
    "        # train on current task\n",
    "        step = 0\n",
    "        for iter in range(num_iter):\n",
    "            batch = trainset.train.next_batch(batch_size)\n",
    "            img = []\n",
    "            label = []\n",
    "            #print batch[1]\n",
    "            #print len(batch[1])\n",
    "            for i in range(len(batch[1])):\n",
    "                if(setnum == 1):\n",
    "                    #print np.where(batch[1][i] == 1)[0][0]\n",
    "                    if(np.where(batch[1][i] == 1)[0][0] <= 3):\n",
    "                        #print \"here\"\n",
    "                        img.append(batch[0][i])\n",
    "                        label.append(batch[1][i])\n",
    "                if(setnum == 2):\n",
    "                    if(np.where(batch[1][i] == 1)[0][0] > 3 and np.where(batch[1][i] == 1)[0][0] <= 6):\n",
    "                        img.append(batch[0][i])\n",
    "                        label.append(batch[1][i])\n",
    "                if(setnum == 3):\n",
    "                    if(np.where(batch[1][i] == 1)[0][0] > 6 ):\n",
    "                        img.append(batch[0][i])\n",
    "                        label.append(batch[1][i])\n",
    "            #print label\n",
    "            #break\n",
    "            model.train_step.run(feed_dict={x: img, y_: label})\n",
    "            #print model.prediction.eval(feed_dict={x: mnist.test.images})\n",
    "            if (iter % disp_freq == 0):\n",
    "                plt.subplot(1, len(lams), l+1)\n",
    "                plots = []\n",
    "                testimg = []\n",
    "                testlabel = []\n",
    "                colors = ['r', 'b', 'g']\n",
    "                for task in range(len(testsets)):\n",
    "                    for i in range(batch_size):\n",
    "                        #print testsets[task].test.images.shape[0]\n",
    "                        if(task == 0):\n",
    "                            if(np.where(testsets[task].test.labels[i] == 1)[0][0] <= 3):\n",
    "                                testimg.append(testsets[task].test.images[i])\n",
    "                                testlabel.append(testsets[task].test.labels[i])\n",
    "                        if(task == 1):\n",
    "                            #print \"setnum2\"\n",
    "                            if(np.where(testsets[task].test.labels[i] == 1)[0][0] > 3 and np.where(testsets[task].test.labels[i] == 1)[0][0] <= 6):\n",
    "                            #if(np.where(testsets[task].test.labels[i] == 1)[0][0] <= 6):\n",
    "                                testimg.append(testsets[task].test.images[i])\n",
    "                                testlabel.append(testsets[task].test.labels[i])\n",
    "                        if(task == 2):\n",
    "                            #print \"setnum3\"\n",
    "                            if(np.where(testsets[task].test.labels[i] == 1)[0][0] > 6):\n",
    "                                testimg.append(testsets[task].test.images[i])\n",
    "                                testlabel.append(testsets[task].test.labels[i])   \n",
    "                    #print testlabel\n",
    "                    feed_dict={x: testimg, y_: testlabel}\n",
    "                    #feed_dict={x: testsets[task].test.images, y_: testsets[task].test.labels}\n",
    "                    test_accs[task][iter/disp_freq] = model.accuracy.eval(feed_dict=feed_dict)\n",
    "                    c = chr(ord('A') + task)\n",
    "                    plot_h, = plt.plot(range(1,iter+2,disp_freq), test_accs[task][:iter/disp_freq+1], colors[task], label=\"task \" + c)\n",
    "                    plots.append(plot_h)\n",
    "                plot_test_acc(plots)\n",
    "                if l == 0: \n",
    "                    plt.title(\"Cross Entropy\")\n",
    "                else:\n",
    "                    plt.title(\"ewc\")\n",
    "                plt.gcf().set_size_inches(len(lams)*5, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD7CAYAAADuFMYYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9d57103790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# train_task(model, 1300, 100, sess, mnist, [mnist], x, y_, lams=[0])\n",
    "train_task_separate(model, 1300, 100, sess, mnist,1, [mnist], x, y_, lams=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "i = 0\n",
    "train_img = mnist.train.images\n",
    "mnist1_img = []\n",
    "mnist1_label = []\n",
    "mnist2_img = []\n",
    "mnist2_label = []\n",
    "mnist3_img = []\n",
    "mnist3_label = []\n",
    "\n",
    "mnist1_test_img = []\n",
    "mnist1_test_label = []\n",
    "mnist2_test_img = []\n",
    "mnist2_test_label = []\n",
    "mnist3_test_img = []\n",
    "mnist3_test_label = []\n",
    "\n",
    "test_img = mnist.test.images\n",
    "\n",
    "while(i < train_img.shape[0]):\n",
    "    batch = mnist.train.next_batch(1)\n",
    "    if (np.where(batch[1] == 1)[1] <= 3):\n",
    "        mnist1_img.append(batch[0])\n",
    "        mnist1_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 3 and np.where(batch[1] == 1)[1] <= 6):\n",
    "        mnist2_img.append(batch[0])\n",
    "        mnist2_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 6):\n",
    "        mnist3_img.append(batch[0])\n",
    "        mnist3_label.append(batch[1])\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "\n",
    "#print mnist1\n",
    "j = 0\n",
    "while(j < test_img.shape[0]):\n",
    "    batch = mnist.test.next_batch(1)\n",
    "    if (np.where(batch[1] == 1)[1] <= 3):\n",
    "        mnist1_test_img.append(batch[0])\n",
    "        mnist1_test_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 3 and np.where(batch[1] == 1)[1] <= 6):\n",
    "        mnist2_test_img.append(batch[0])\n",
    "        mnist2_test_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 6):\n",
    "        mnist3_test_img.append(batch[0])\n",
    "        mnist3_test_label.append(batch[1])\n",
    "    j+=1\n",
    "    \n",
    "    \n",
    "# mnist1_t=zip(mnist1_test_img,mnist1_test_label)\n",
    "# mnist2_t=zip(mnist2_test_img,mnist2_test_label)\n",
    "# mnist3_t=zip(mnist1_test_img,mnist3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_fisher(mnist.validation.images, sess, num_samples=200, plot_diffs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist2 = permute_mnist(mnist)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "mnist_imshow(mnist.train.images[5])\n",
    "plt.title(\"original task image\")\n",
    "plt.subplot(1,2,2)\n",
    "mnist_imshow(mnist2.train.images[5])\n",
    "plt.title(\"new task image\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.star()\n",
    "#train_task(model, 1300, 100,sess, mnist2, [mnist, mnist2], x, y_, lams=[0, 500])\n",
    "train_task_separate(model, 1300, 100,sess, mnist,2, [mnist, mnist], x, y_, lams=[0, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compute_fisher(mnist2.validation.images, sess, num_samples=200, plot_diffs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist3 = permute_mnist(mnist)\n",
    "model.star()\n",
    "train_task(model, 1300, 100,sess, mnist3, [mnist, mnist2, mnist3], x, y_, lams=[0, 1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_iter = 4000\n",
    "# display_step = 100\n",
    "# print model.train_step\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "#     for iter in range(num_iter):\n",
    "#         batch = mnist.train.next_batch(batch_size)\n",
    "#         feed_dict={x: batch[0], y_: batch[1]}\n",
    "#         model.train_step.run(feed_dict=feed_dict)\n",
    "        \n",
    "#         if num_iter % display_step == 0:\n",
    "#             model.updateM(iter)\n",
    "#             # Calculate batch loss and accuracy\n",
    "#     #         loss, acc = sess.run([model.cross_entropy, model.accuracy], feed_dict={x: batch[0],\n",
    "#     #                                                       y_: batch[1],\n",
    "#     #                                                       keep_prob: 1.})\n",
    "#             acc = model.accuracy.eval(feed_dict=feed_dict)\n",
    "#             print model.cross_entropy.eval(feed_dict=feed_dict)\n",
    "#     #         print(\"Iter \" + str(iter*batch_size) + \", Minibatch Loss= \" + \\\n",
    "#     #               \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "#     #               \"{:.5f}\".format(acc))\n",
    "#             print \"Iter\" + str(iter*batch_size) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "\n",
    "\n",
    "#     print(\"Optimization Finished!\")\n",
    "\n",
    "#         # Calculate accuracy for 256 mnist test images\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
