{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2.1\n"
     ]
    }
   ],
   "source": [
    "print tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 100\n",
    "disp_freq = 100\n",
    "\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "# Create model\n",
    "def conv_net(x, weights, biases):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "class Model:\n",
    "\n",
    "\n",
    "    def __init__(self, x, y_):\n",
    "\n",
    "        self.x = x # input placeholder\n",
    "    \n",
    "#         weights = {\n",
    "#         # 5x5 conv, 1 input, 32 outputs\n",
    "#         'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32],stddev=0.001)),\n",
    "#         # 5x5 conv, 32 inputs, 64 outputs\n",
    "#         'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64],stddev=0.001)),\n",
    "#         # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "#         'wd1': tf.Variable(tf.truncated_normal([7*7*64, 1024],stddev=0.001)),\n",
    "#         # 1024 inputs, 10 outputs (class prediction)\n",
    "#         'out': tf.Variable(tf.truncated_normal([1024, n_classes],stddev=0.001))\n",
    "#         }\n",
    "\n",
    "#         biases = {\n",
    "#         'bc1': tf.Variable(tf.truncated_normal([32],stddev=0.001)),\n",
    "#         'bc2': tf.Variable(tf.truncated_normal([64],stddev=0.001)),\n",
    "#         'bd1': tf.Variable(tf.truncated_normal([1024],stddev=0.001)),\n",
    "#         'out': tf.Variable(tf.truncated_normal([n_classes],stddev=0.001))\n",
    "#         }\n",
    "       \n",
    "        \n",
    "#         self.y = conv_net(x, weights, biases)\n",
    "\n",
    "#         self.var_list=[weights['wc1'],weights['wc2'],weights['wd1'],weights['out'],\n",
    "#                        biases['bc1'],biases['bc2'],biases['bd1'],biases['out']]\n",
    "\n",
    "        # simple 2-layer network\n",
    "        in_dim = int(x.get_shape()[1]) # 784 for MNIST\n",
    "        out_dim = int(y_.get_shape()[1]) # 10 for MNIST\n",
    "\n",
    "        W1 = weight_variable([in_dim,50])\n",
    "        b1 = bias_variable([50])\n",
    "\n",
    "        W2 = weight_variable([50,out_dim])\n",
    "        b2 = bias_variable([out_dim])\n",
    "\n",
    "        h1 = tf.nn.relu(tf.matmul(x,W1) + b1) # hidden layer    \n",
    "        self.y = tf.matmul(h1,W2) + b2\n",
    "        self.var_list = [W1, b1, W2, b2]\n",
    "        \n",
    "        \n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y,labels=y_))\n",
    "\n",
    "        self.set_vanilla_loss()\n",
    "        \n",
    "        \n",
    "        correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(y_,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "\n",
    "    def compute_fisher(self, imgset, sess, num_samples=200, plot_diffs=False, disp_freq=10):\n",
    "        # compute Fisher information for each parameter\n",
    "\n",
    "        # initialize Fisher information for most recent task\n",
    "        self.F_accum = []\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.F_accum.append(np.zeros(self.var_list[v].get_shape().as_list()))\n",
    "\n",
    "        # sampling a random class from softmax\n",
    "        probs = tf.nn.softmax(self.y)\n",
    "        class_ind = tf.to_int32(tf.multinomial(tf.log(probs), 1)[0][0])\n",
    "\n",
    "        if(plot_diffs):\n",
    "            # track differences in mean Fisher info\n",
    "            F_prev = deepcopy(self.F_accum)\n",
    "            mean_diffs = np.zeros(0)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # select random input image\n",
    "            im_ind = np.random.randint(imgset.shape[0])\n",
    "            # compute first-order derivatives\n",
    "            ders = sess.run(tf.gradients(tf.log(probs[0,class_ind]), self.var_list), feed_dict={self.x: imgset[im_ind:im_ind+1]})\n",
    "            # square the derivatives and add to total\n",
    "            for v in range(len(self.F_accum)):\n",
    "                self.F_accum[v] += np.square(ders[v])\n",
    "            if(plot_diffs):\n",
    "                if i % disp_freq == 0 and i > 0:\n",
    "                    # recording mean diffs of F\n",
    "                    F_diff = 0\n",
    "                    for v in range(len(self.F_accum)):\n",
    "                        F_diff += np.sum(np.absolute(self.F_accum[v]/(i+1) - F_prev[v]))\n",
    "                    mean_diff = np.mean(F_diff)\n",
    "                    mean_diffs = np.append(mean_diffs, mean_diff)\n",
    "                    for v in range(len(self.F_accum)):\n",
    "                        F_prev[v] = self.F_accum[v]/(i+1)\n",
    "                    plt.plot(range(disp_freq+1, i+2, disp_freq), mean_diffs)\n",
    "                    plt.xlabel(\"Number of samples\")\n",
    "                    plt.ylabel(\"Mean absolute Fisher difference\")\n",
    "                    display.display(plt.gcf())\n",
    "                    display.clear_output(wait=True)\n",
    "\n",
    "        # divide totals by number of samples\n",
    "        for v in range(len(self.F_accum)):\n",
    "            self.F_accum[v] /= num_samples\n",
    "    \n",
    "    def set_vanilla_loss(self):\n",
    "        self.train_step = tf.train.AdamOptimizer(0.01).minimize(self.cross_entropy)\n",
    "\n",
    "    def star(self):\n",
    "    # used for saving optimal weights after most recent task training\n",
    "        self.star_vars = []\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.star_vars.append(self.var_list[v].eval())\n",
    "    def update_ewc_loss(self, lam):\n",
    "        # elastic weight consolidation\n",
    "        # lam is weighting for previous task(s) constraints\n",
    "\n",
    "        if not hasattr(self, \"ewc_loss\"):\n",
    "            self.ewc_loss = self.cross_entropy\n",
    "\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.ewc_loss += (lam/2) * tf.reduce_sum(tf.multiply(self.F_accum[v].astype(np.float32),tf.square(self.var_list[v] - self.star_vars[v])))\n",
    "        self.train_step = tf.train.AdamOptimizer(0.01).minimize(self.ewc_loss)\n",
    "    def restore(self, sess):\n",
    "    # reassign optimal weights for latest task\n",
    "        if hasattr(self, \"star_vars\"):\n",
    "            for v in range(len(self.var_list)):\n",
    "                sess.run(self.var_list[v].assign(self.star_vars[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Model(x, y_)\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_imshow(img):\n",
    "    plt.imshow(img.reshape([28,28]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "def permute_mnist(mnist):\n",
    "    perm_inds = range(mnist.train.images.shape[1])\n",
    "    np.random.shuffle(perm_inds)\n",
    "    mnist2 = deepcopy(mnist)\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "    for set_name in sets:\n",
    "        this_set = getattr(mnist2, set_name) # shallow copy\n",
    "        this_set._images = np.transpose(np.array([this_set.images[:,c] for c in perm_inds]))\n",
    "    return mnist2\n",
    "\n",
    "def plot_test_acc(plot_handles):\n",
    "    plt.legend(handles=plot_handles, loc=\"center right\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.ylim(0,1)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_task(model, num_iter, disp_freq,sess, trainset, testsets, x, y_, lams=[0]):\n",
    "    for l in range(len(lams)):\n",
    "        # lams[l] sets weight on old task(s)\n",
    "        temp = set(tf.global_variables())\n",
    "        model.restore(sess) # reassign optimal weights from previous training session\n",
    "        if(lams[l] == 0):\n",
    "            model.set_vanilla_loss()\n",
    "        else:\n",
    "            model.update_ewc_loss(lams[l])\n",
    "            #model.train_step = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        # initialize test accuracy array for each task \n",
    "        #Optimizer = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))\n",
    "        \n",
    "        test_accs = []\n",
    "        for task in range(len(testsets)):\n",
    "            test_accs.append(np.zeros(num_iter/disp_freq))\n",
    "        # train on current task\n",
    "        step = 0\n",
    "        for iter in range(num_iter):\n",
    "            batch = trainset.train.next_batch(batch_size)\n",
    "            model.train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            #print model.prediction.eval(feed_dict={x: mnist.test.images})\n",
    "            if (iter % disp_freq == 0):\n",
    "                plt.subplot(1, len(lams), l+1)\n",
    "                plots = []\n",
    "                colors = ['r', 'b', 'g']\n",
    "                for task in range(len(testsets)):\n",
    "                    feed_dict={x: testsets[task].test.images, y_: testsets[task].test.labels}\n",
    "                    test_accs[task][iter/disp_freq] = model.accuracy.eval(feed_dict=feed_dict)\n",
    "                    c = chr(ord('A') + task)\n",
    "                    plot_h, = plt.plot(range(1,iter+2,disp_freq), test_accs[task][:iter/disp_freq+1], colors[task], label=\"task \" + c)\n",
    "                    plots.append(plot_h)\n",
    "                plot_test_acc(plots)\n",
    "                if l == 0: \n",
    "                    plt.title(\"Cross Entropy\")\n",
    "                else:\n",
    "                    plt.title(\"ewc\")\n",
    "                plt.gcf().set_size_inches(len(lams)*5, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_task_separate(model, num_iter, disp_freq,sess, trainset,setnum, testsets, x, y_, lams=[0]):\n",
    "    for l in range(len(lams)):\n",
    "        # lams[l] sets weight on old task(s)\n",
    "        temp = set(tf.global_variables())\n",
    "        model.restore(sess) # reassign optimal weights from previous training session\n",
    "        if(lams[l] == 0):\n",
    "            model.set_vanilla_loss()\n",
    "        else:\n",
    "            model.update_ewc_loss(lams[l])\n",
    "            #model.train_step = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        # initialize test accuracy array for each task \n",
    "        #Optimizer = tf.train.AdamOptimizer(0.01).minimize(model.ewc_loss)\n",
    "        sess.run(tf.variables_initializer(set(tf.global_variables()) - temp))\n",
    "        \n",
    "        test_accs = []\n",
    "        for task in range(len(testsets)):\n",
    "            test_accs.append(np.zeros(num_iter/disp_freq))\n",
    "        # train on current task\n",
    "        step = 0\n",
    "        for iter in range(num_iter):\n",
    "            batch = trainset.train.next_batch(batch_size)\n",
    "            img = []\n",
    "            label = []\n",
    "            #print batch[1]\n",
    "            #print len(batch[1])\n",
    "            for i in range(len(batch[1])):\n",
    "                if(setnum == 1):\n",
    "                    #print np.where(batch[1][i] == 1)[0][0]\n",
    "                    if(np.where(batch[1][i] == 1)[0][0] <= 3):\n",
    "                        #print \"here\"\n",
    "                        img.append(batch[0][i])\n",
    "                        label.append(batch[1][i])\n",
    "                if(setnum == 2):\n",
    "                    if(np.where(batch[1][i] == 1)[0][0] > 3 and np.where(batch[1][i] == 1)[0][0] <= 6):\n",
    "                        img.append(batch[0][i])\n",
    "                        label.append(batch[1][i])\n",
    "                if(setnum == 3):\n",
    "                    if(np.where(batch[1][i] == 1)[0][0] > 6 ):\n",
    "                        img.append(batch[0][i])\n",
    "                        label.append(batch[1][i])\n",
    "            #print label\n",
    "            #break\n",
    "            model.train_step.run(feed_dict={x: img, y_: label})\n",
    "            #print model.prediction.eval(feed_dict={x: mnist.test.images})\n",
    "            if (iter % disp_freq == 0):\n",
    "                plt.subplot(1, len(lams), l+1)\n",
    "                plots = []\n",
    "                testimg = []\n",
    "                testlabel = []\n",
    "                colors = ['r', 'b', 'g']\n",
    "                for task in range(len(testsets)):\n",
    "                    for i in range(batch_size):\n",
    "                        #print testsets[task].test.images.shape[0]\n",
    "                        if(task == 0):\n",
    "                            if(np.where(testsets[task].test.labels[i] == 1)[0][0] <= 3):\n",
    "                                testimg.append(testsets[task].test.images[i])\n",
    "                                testlabel.append(testsets[task].test.labels[i])\n",
    "                        if(task == 1):\n",
    "                            #print \"setnum2\"\n",
    "                            if(np.where(testsets[task].test.labels[i] == 1)[0][0] > 3 and np.where(testsets[task].test.labels[i] == 1)[0][0] <= 6):\n",
    "                            #if(np.where(testsets[task].test.labels[i] == 1)[0][0] <= 6):\n",
    "                                testimg.append(testsets[task].test.images[i])\n",
    "                                testlabel.append(testsets[task].test.labels[i])\n",
    "                        if(task == 2):\n",
    "                            #print \"setnum3\"\n",
    "                            if(np.where(testsets[task].test.labels[i] == 1)[0][0] > 6):\n",
    "                                testimg.append(testsets[task].test.images[i])\n",
    "                                testlabel.append(testsets[task].test.labels[i])   \n",
    "                    #print testlabel\n",
    "                    feed_dict={x: testimg, y_: testlabel}\n",
    "                    #feed_dict={x: testsets[task].test.images, y_: testsets[task].test.labels}\n",
    "                    test_accs[task][iter/disp_freq] = model.accuracy.eval(feed_dict=feed_dict)\n",
    "                    c = chr(ord('A') + task)\n",
    "                    plot_h, = plt.plot(range(1,iter+2,disp_freq), test_accs[task][:iter/disp_freq+1], colors[task], label=\"task \" + c)\n",
    "                    plots.append(plot_h)\n",
    "                plot_test_acc(plots)\n",
    "                if l == 0: \n",
    "                    plt.title(\"Cross Entropy\")\n",
    "                else:\n",
    "                    plt.title(\"ewc\")\n",
    "                plt.gcf().set_size_inches(len(lams)*5, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU0AAAD7CAYAAADuFMYYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb3dc6fba10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "# train_task(model, 1300, 100, sess, mnist, [mnist], x, y_, lams=[0])\n",
    "train_task_separate(model, 1300, 100, sess, mnist,1, [mnist], x, y_, lams=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "i = 0\n",
    "train_img = mnist.train.images\n",
    "mnist1_img = []\n",
    "mnist1_label = []\n",
    "mnist2_img = []\n",
    "mnist2_label = []\n",
    "mnist3_img = []\n",
    "mnist3_label = []\n",
    "\n",
    "mnist1_test_img = []\n",
    "mnist1_test_label = []\n",
    "mnist2_test_img = []\n",
    "mnist2_test_label = []\n",
    "mnist3_test_img = []\n",
    "mnist3_test_label = []\n",
    "\n",
    "test_img = mnist.test.images\n",
    "\n",
    "while(i < train_img.shape[0]):\n",
    "    batch = mnist.train.next_batch(1)\n",
    "    if (np.where(batch[1] == 1)[1] <= 3):\n",
    "        mnist1_img.append(batch[0])\n",
    "        mnist1_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 3 and np.where(batch[1] == 1)[1] <= 6):\n",
    "        mnist2_img.append(batch[0])\n",
    "        mnist2_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 6):\n",
    "        mnist3_img.append(batch[0])\n",
    "        mnist3_label.append(batch[1])\n",
    "    i+=1\n",
    "    \n",
    "\n",
    "\n",
    "#print mnist1\n",
    "j = 0\n",
    "while(j < test_img.shape[0]):\n",
    "    batch = mnist.test.next_batch(1)\n",
    "    if (np.where(batch[1] == 1)[1] <= 3):\n",
    "        mnist1_test_img.append(batch[0])\n",
    "        mnist1_test_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 3 and np.where(batch[1] == 1)[1] <= 6):\n",
    "        mnist2_test_img.append(batch[0])\n",
    "        mnist2_test_label.append(batch[1])\n",
    "    if (np.where(batch[1] == 1)[1] > 6):\n",
    "        mnist3_test_img.append(batch[0])\n",
    "        mnist3_test_label.append(batch[1])\n",
    "    j+=1\n",
    "    \n",
    "\n",
    "# mnist1_t=zip(mnist1_test_img,mnist1_test_label)\n",
    "# mnist2_t=zip(mnist2_test_img,mnist2_test_label)\n",
    "# mnist3_t=zip(mnist1_test_img,mnist3_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import DataSet\n",
    "from tensorflow.python.framework import dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshape = True\n",
    "dtype=dtypes.float32\n",
    "\n",
    "options = dict(dtype=dtype,reshape=reshape,seed = None)\n",
    "\n",
    "def mnistreshape(inputdata):\n",
    "    inputdata = np.array(inputdata)\n",
    "    return inputdata.reshape(inputdata.shape[0],28,28,1)\n",
    "\n",
    "mnist1_img = mnistreshape(mnist1_img)\n",
    "mnist2_img = mnistreshape(mnist2_img)\n",
    "mnist3_img = mnistreshape(mnist3_img)\n",
    "mnist1_test_img = mnistreshape(mnist1_test_img)\n",
    "mnist2_test_img = mnistreshape(mnist2_test_img)\n",
    "mnist3_test_img = mnistreshape(mnist3_test_img)\n",
    "\n",
    "mnist1_label = np.array(mnist1_label)\n",
    "mnist2_label = np.array(mnist2_label)\n",
    "mnist3_label = np.array(mnist3_label)\n",
    "mnist1_test_label = np.array(mnist1_test_label)\n",
    "mnist2_test_label = np.array(mnist2_test_label)\n",
    "mnist3_test_label = np.array(mnist3_test_label)\n",
    "\n",
    "mnist1 = DataSet(mnist1_img,mnist1_label,**options)\n",
    "mnist2 = DataSet(mnist2_img,mnist2_label,**options)\n",
    "mnist3 = DataSet(mnist3_img,mnist3_label,**options)\n",
    "mnist1_t = DataSet(mnist1_test_img,mnist1_test_label,**options)\n",
    "mnist2_t = DataSet(mnist2_test_img,mnist2_test_label,**options)\n",
    "mnist3_t = DataSet(mnist3_test_img,mnist3_test_label,**options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
