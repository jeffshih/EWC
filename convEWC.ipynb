{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_iters = 200000\n",
    "batch_size = 100\n",
    "disp_freq = 100\n",
    "\n",
    "n_input = 784 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 10 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y_ = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out\n",
    "\n",
    "class Model:\n",
    "\n",
    "\n",
    "    def __init__(self, x, y_):\n",
    "\n",
    "#         self.x = x # input placeholder\n",
    "    \n",
    "#         weights = {\n",
    "#     # 5x5 conv, 1 input, 32 outputs\n",
    "#         'wc1': tf.Variable(tf.truncated_normal([5, 5, 1, 32],stddev=0.001)),\n",
    "#         # 5x5 conv, 32 inputs, 64 outputs\n",
    "#         'wc2': tf.Variable(tf.truncated_normal([5, 5, 32, 64],stddev=0.001)),\n",
    "#         # fully connected, 7*7*64 inputs, 1024 outputs\n",
    "#         'wd1': tf.Variable(tf.truncated_normal([7*7*64, 1024],stddev=0.001)),\n",
    "#         # 1024 inputs, 10 outputs (class prediction)\n",
    "#         'out': tf.Variable(tf.truncated_normal([1024, n_classes],stddev=0.001))\n",
    "#         }\n",
    "\n",
    "#         biases = {\n",
    "#         'bc1': tf.Variable(tf.truncated_normal([32],stddev=0.001)),\n",
    "#         'bc2': tf.Variable(tf.truncated_normal([64],stddev=0.001)),\n",
    "#         'bd1': tf.Variable(tf.truncated_normal([1024],stddev=0.001)),\n",
    "#         'out': tf.Variable(tf.truncated_normal([n_classes],stddev=0.001))\n",
    "#         }\n",
    "       \n",
    "        \n",
    "\n",
    "#         fc1 = tf.nn.dropout(fc1, dropout)\n",
    "#         self.y = conv_net(x, weights, biases)\n",
    "\n",
    "#         self.var_list=[weights['wc1'],weights['wc2'],weights['wd1'],weights['out'],\n",
    "#                        biases['bc1'],biases['bc2'],biases['bd1'],biases['out']]\n",
    "\n",
    "\n",
    "#         \n",
    "        \n",
    "#         \n",
    "#         self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.y,labels=y_))\n",
    "#         self.ewc_loss = self.cross_entropy\n",
    "#         self.train_step = tf.train.AdamOptimizer(0.001).minimize(self.ewc_loss)\n",
    "        \n",
    "#         correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(y_,1))\n",
    "#         self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "\n",
    "        in_dim = int(x.get_shape()[1]) # 784 for MNIST\n",
    "        out_dim = int(y_.get_shape()[1]) # 10 for MNIST\n",
    "\n",
    "        self.x = x # input placeholder\n",
    "\n",
    "        # simple 2-layer network\n",
    "        W1 = weight_variable([in_dim,50])\n",
    "        b1 = bias_variable([50])\n",
    "\n",
    "        W2 = weight_variable([50,out_dim])\n",
    "        b2 = bias_variable([out_dim])\n",
    "\n",
    "        h1 = tf.nn.relu(tf.matmul(x,W1) + b1) # hidden layer\n",
    "        self.y = tf.matmul(h1,W2) + b2 # output layer\n",
    "\n",
    "        self.var_list = [W1, b1, W2, b2]\n",
    "        \n",
    "       \n",
    "        # vanilla single-task loss\n",
    "        self.cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=self.y))\n",
    "        if not hasattr(self, \"ewc_loss\"):\n",
    "            print \"origin cross entropy\"\n",
    "            self.ewc_loss = self.cross_entropy\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(0.01).minimize(self.ewc_loss)\n",
    "        # performance metrics\n",
    "        correct_prediction = tf.equal(tf.argmax(self.y,1), tf.argmax(y_,1))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        \n",
    "    def compute_fisher(self, imgset, sess, num_samples=200, plot_diffs=False, disp_freq=10):\n",
    "        # computer Fisher information for each parameter\n",
    "\n",
    "        # initialize Fisher information for most recent task\n",
    "        self.F_accum = []\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.F_accum.append(np.zeros(self.var_list[v].get_shape().as_list()))\n",
    "\n",
    "        # sampling a random class from softmax\n",
    "        probs = tf.nn.softmax(self.y)\n",
    "        class_ind = tf.to_int32(tf.multinomial(tf.log(probs), 1)[0][0])\n",
    "\n",
    "        if(plot_diffs):\n",
    "            # track differences in mean Fisher info\n",
    "            F_prev = deepcopy(self.F_accum)\n",
    "            mean_diffs = np.zeros(0)\n",
    "\n",
    "        for i in range(num_samples):\n",
    "            # select random input image\n",
    "            im_ind = np.random.randint(imgset.shape[0])\n",
    "            # compute first-order derivatives\n",
    "            ders = sess.run(tf.gradients(tf.log(probs[0,class_ind]), self.var_list), feed_dict={self.x: imgset[im_ind:im_ind+1]})\n",
    "            # square the derivatives and add to total\n",
    "            for v in range(len(self.F_accum)):\n",
    "                self.F_accum[v] += np.square(ders[v])\n",
    "            if(plot_diffs):\n",
    "                if i % disp_freq == 0 and i > 0:\n",
    "                    # recording mean diffs of F\n",
    "                    F_diff = 0\n",
    "                    for v in range(len(self.F_accum)):\n",
    "                        F_diff += np.sum(np.absolute(self.F_accum[v]/(i+1) - F_prev[v]))\n",
    "                    mean_diff = np.mean(F_diff)\n",
    "                    mean_diffs = np.append(mean_diffs, mean_diff)\n",
    "                    for v in range(len(self.F_accum)):\n",
    "                        F_prev[v] = self.F_accum[v]/(i+1)\n",
    "                    plt.plot(range(disp_freq+1, i+2, disp_freq), mean_diffs)\n",
    "                    plt.xlabel(\"Number of samples\")\n",
    "                    plt.ylabel(\"Mean absolute Fisher difference\")\n",
    "                    display.display(plt.gcf())\n",
    "                    display.clear_output(wait=True)\n",
    "\n",
    "        # divide totals by number of samples\n",
    "        for v in range(len(self.F_accum)):\n",
    "            self.F_accum[v] /= num_samples\n",
    "    \n",
    "    def set_vanilla_loss(self):\n",
    "        self.train_step = tf.train.GradientDescentOptimizer(0.01).minimize(self.cross_entropy)\n",
    "\n",
    "    def star(self):\n",
    "    # used for saving optimal weights after most recent task training\n",
    "        self.star_vars = []\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.star_vars.append(self.var_list[v].eval())\n",
    "    def update_ewc_loss(self, lam):\n",
    "        # elastic weight consolidation\n",
    "        # lam is weighting for previous task(s) constraints\n",
    "\n",
    "        if not hasattr(self, \"ewc_loss\"):\n",
    "            self.ewc_loss = self.cross_entropy\n",
    "\n",
    "        for v in range(len(self.var_list)):\n",
    "            self.ewc_loss += (lam/2) * tf.reduce_sum(tf.multiply(self.F_accum[v].astype(np.float32),tf.square(self.var_list[v] - self.star_vars[v])))\n",
    "       \n",
    "    def restore(self, sess):\n",
    "    # reassign optimal weights for latest task\n",
    "        if hasattr(self, \"star_vars\"):\n",
    "            for v in range(len(self.var_list)):\n",
    "                sess.run(self.var_list[v].assign(self.star_vars[v]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(x, y_)\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mnist_imshow(img):\n",
    "    plt.imshow(img.reshape([28,28]), cmap=\"gray\")\n",
    "    plt.axis('off')\n",
    "\n",
    "def permute_mnist(mnist):\n",
    "    perm_inds = range(mnist.train.images.shape[1])\n",
    "    np.random.shuffle(perm_inds)\n",
    "    mnist2 = deepcopy(mnist)\n",
    "    sets = [\"train\", \"validation\", \"test\"]\n",
    "    for set_name in sets:\n",
    "        this_set = getattr(mnist2, set_name) # shallow copy\n",
    "        this_set._images = np.transpose(np.array([this_set.images[:,c] for c in perm_inds]))\n",
    "    return mnist2\n",
    "\n",
    "def plot_test_acc(plot_handles):\n",
    "    plt.legend(handles=plot_handles, loc=\"center right\")\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    plt.ylabel(\"Test Accuracy\")\n",
    "    plt.ylim(0,1)\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_task(model, num_iter, disp_freq, trainset, testsets, x, y_, lams=[0]):\n",
    "    for l in range(len(lams)):\n",
    "        # lams[l] sets weight on old task(s)\n",
    "        model.restore(sess) # reassign optimal weights from previous training session\n",
    "        if(lams[l] != 0):\n",
    "            #model.set_vanilla_loss()\n",
    "        #else:\n",
    "            model.update_ewc_loss(lams[l])\n",
    "        # initialize test accuracy array for each task \n",
    "        test_accs = []\n",
    "        for task in range(len(testsets)):\n",
    "            test_accs.append(np.zeros(num_iter/disp_freq))\n",
    "        # train on current task\n",
    "        step = 0\n",
    "        for iter in range(num_iter):\n",
    "            batch = trainset.train.next_batch(batch_size)\n",
    "            model.train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            #print model.prediction.eval(feed_dict={x: mnist.test.images})\n",
    "            if (iter % disp_freq == 0):\n",
    "                plt.subplot(1, len(lams), l+1)\n",
    "                plots = []\n",
    "                colors = ['r', 'b', 'g']\n",
    "                for task in range(len(testsets)):\n",
    "                    feed_dict={x: testsets[task].test.images, y_: testsets[task].test.labels}\n",
    "                    test_accs[task][iter/disp_freq] = model.accuracy.eval(feed_dict=feed_dict)\n",
    "                    c = chr(ord('A') + task)\n",
    "                    plot_h, = plt.plot(range(1,iter+2,disp_freq), test_accs[task][:iter/disp_freq+1], colors[task], label=\"task \" + c)\n",
    "                    plots.append(plot_h)\n",
    "                plot_test_acc(plots)\n",
    "                if l == 0: \n",
    "                    plt.title(\"sgd\")\n",
    "                else:\n",
    "                    plt.title(\"ewc\")\n",
    "                plt.gcf().set_size_inches(len(lams)*5, 3.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train_task(model, 2000, 100, mnist, [mnist], x, y_, lams=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_fisher(mnist.validation.images, sess, num_samples=200, plot_diffs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist2 = permute_mnist(mnist)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "mnist_imshow(mnist.train.images[5])\n",
    "plt.title(\"original task image\")\n",
    "plt.subplot(1,2,2)\n",
    "mnist_imshow(mnist2.train.images[5])\n",
    "plt.title(\"new task image\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.star()\n",
    "train_task(model, 1200, 20, mnist2, [mnist, mnist2], x, y_, lams=[0, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compute_fisher(mnist2.validation.images, sess, num_samples=200, plot_diffs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mnist3 = permute_mnist(mnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.star()\n",
    "train_task(model, 1200, 20, mnist3, [mnist, mnist2, mnist3], x, y_, lams=[0, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
